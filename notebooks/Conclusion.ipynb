{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConclusiveNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQjoC7T08Gd_"
      },
      "source": [
        "## KNOWLEDGE DISTILLATION\n",
        "Just for completeness in this notebook we are going to show how to use various techniques of knowledge distillation, that are shonw in the bibliography's papers. All of these techniques are compatible with the method written during the work and I'm going to show how."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUFDtlAfmOXn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from WideResNet import WideResidualNetwork\n",
        "from knowledge_distillation import *\n",
        "from BANEnsemble import BANEnsemble\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G_pdDFcvc_E",
        "outputId": "daed9552-dff1-4ff3-ed10-293b36dee150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vau7FBDvqhm"
      },
      "source": [
        "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW8hfwKKv7zT"
      },
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQG1ZXY00QQS"
      },
      "source": [
        "## Dataset check\n",
        "\n",
        "It's a good idea to check if the dataset is balanced in order to know if accuracy can be a valid metric, so let's plot it. And in the next cells I'll build train, validation and test datasets, with a minimal image augmentation. Then I'll train the teacher on them as usual, nothing different from previous tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIQmsa82wyqK",
        "outputId": "022b30dd-f856-41a8-a128-9dc7e01b7e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Let's show an histogram in order to show if the test set is balanced and accuracy can be a valid metric\n",
        "plt.hist(np.argmax(y_test, axis=1))\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOxklEQVR4nO3dYYydVZ3H8e9vqaBglhaYNDit225sNMTEhUywho0x1LiAxvJCDcYsjWnSN+yKYqJ19wXZ3TeSGFGSDUlD0bIxrG4lS6NGwxaM2Rd0bdUgUF1GFNum0FGguhqjjf99cQ+719qxzNyZO3LP95Pc3Oec5zzPc07P5HfvnHnubaoKSVIf/mSlOyBJGh9DX5I6YuhLUkcMfUnqiKEvSR1ZtdId+EMuueSS2rBhw0p3Q5JeUg4dOvSTqpo6074/6tDfsGEDBw8eXOluSNJLSpKn5tvn8o4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyFlDP8ndSU4keXSo7qIkDyR5oj2vafVJckeS2SSPJLli6Jhtrf0TSbYtz3AkSX/Ii3mn/1ngmtPqdgL7q2oTsL+VAa4FNrXHDuBOGLxIALcCbwSuBG594YVCkjQ+Zw39qvoG8Oxp1VuBPW17D3D9UP09NfAwsDrJpcBfAQ9U1bNV9RzwAL//QiJJWmaL/UTu2qo63rafBta27WngyFC7o61uvvrfk2QHg98SePWrX73I7g1s2PnlkY6XpJXyo4+/fVnOO/IfcmvwX28t2X+/VVW7qmqmqmamps741RGSpEVabOg/05ZtaM8nWv0xYP1Qu3Wtbr56SdIYLTb09wEv3IGzDbh/qP7GdhfPZuBkWwb6GvC2JGvaH3Df1uokSWN01jX9JPcCbwEuSXKUwV04Hwe+kGQ78BTwntb8K8B1wCzwS+D9AFX1bJJ/Ar7Z2v1jVZ3+x2FJ0jI7a+hX1Xvn2bXlDG0LuGme89wN3L2g3kmSlpSfyJWkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZKTQT/KhJI8leTTJvUlenmRjkgNJZpN8Psm5re15rTzb9m9YigFIkl68RYd+kmngA8BMVb0eOAe4AbgNuL2qXgM8B2xvh2wHnmv1t7d2kqQxGnV5ZxXwiiSrgPOB48DVwN62fw9wfdve2sq0/VuSZMTrS5IWYNGhX1XHgE8AP2YQ9ieBQ8DzVXWqNTsKTLftaeBIO/ZUa3/x6edNsiPJwSQH5+bmFts9SdIZjLK8s4bBu/eNwKuAC4BrRu1QVe2qqpmqmpmamhr1dJKkIaMs77wV+GFVzVXVb4D7gKuA1W25B2AdcKxtHwPWA7T9FwI/HeH6kqQFGiX0fwxsTnJ+W5vfAjwOPAS8q7XZBtzftve1Mm3/g1VVI1xfkrRAo6zpH2DwB9lvAd9t59oFfBS4JcksgzX73e2Q3cDFrf4WYOcI/ZYkLcKqszeZX1XdCtx6WvWTwJVnaPsr4N2jXE+SNBo/kStJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyEihn2R1kr1JvpfkcJI3JbkoyQNJnmjPa1rbJLkjyWySR5JcsTRDkCS9WKO+0/808NWqeh3wBuAwsBPYX1WbgP2tDHAtsKk9dgB3jnhtSdICLTr0k1wIvBnYDVBVv66q54GtwJ7WbA9wfdveCtxTAw8Dq5NcuuieS5IWbJR3+huBOeAzSb6d5K4kFwBrq+p4a/M0sLZtTwNHho4/2uokSWMySuivAq4A7qyqy4Ff8P9LOQBUVQG1kJMm2ZHkYJKDc3NzI3RPknS6UUL/KHC0qg608l4GLwLPvLBs055PtP3HgPVDx69rdb+jqnZV1UxVzUxNTY3QPUnS6RYd+lX1NHAkyWtb1RbgcWAfsK3VbQPub9v7gBvbXTybgZNDy0CSpDFYNeLxfwt8Lsm5wJPA+xm8kHwhyXbgKeA9re1XgOuAWeCXra0kaYxGCv2q+g4wc4ZdW87QtoCbRrmeJGk0fiJXkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6MnLoJzknybeTfKmVNyY5kGQ2yeeTnNvqz2vl2bZ/w6jXliQtzFK8078ZODxUvg24vapeAzwHbG/124HnWv3trZ0kaYxGCv0k64C3A3e1coCrgb2tyR7g+ra9tZVp+7e09pKkMRn1nf6ngI8Av23li4Hnq+pUKx8Fptv2NHAEoO0/2dr/jiQ7khxMcnBubm7E7kmShi069JO8AzhRVYeWsD9U1a6qmqmqmampqaU8tSR1b9UIx14FvDPJdcDLgT8FPg2sTrKqvZtfBxxr7Y8B64GjSVYBFwI/HeH6kqQFWvQ7/ar6WFWtq6oNwA3Ag1X1PuAh4F2t2Tbg/ra9r5Vp+x+sqlrs9SVJC7cc9+l/FLglySyDNfvdrX43cHGrvwXYuQzXliT9AaMs7/yfqvo68PW2/SRw5Rna/Ap491JcT5K0OH4iV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSRRYd+kvVJHkryeJLHktzc6i9K8kCSJ9rzmlafJHckmU3ySJIrlmoQkqQXZ5R3+qeAD1fVZcBm4KYklwE7gf1VtQnY38oA1wKb2mMHcOcI15YkLcKiQ7+qjlfVt9r2z4HDwDSwFdjTmu0Brm/bW4F7auBhYHWSSxfdc0nSgi3Jmn6SDcDlwAFgbVUdb7ueBta27WngyNBhR1vd6efakeRgkoNzc3NL0T1JUjNy6Cd5JfBF4INV9bPhfVVVQC3kfFW1q6pmqmpmampq1O5JkoaMFPpJXsYg8D9XVfe16mdeWLZpzyda/TFg/dDh61qdJGlMRrl7J8Bu4HBVfXJo1z5gW9veBtw/VH9ju4tnM3ByaBlIkjQGq0Y49irgr4HvJvlOq/s74OPAF5JsB54C3tP2fQW4DpgFfgm8f4RrS5IWYdGhX1X/CWSe3VvO0L6AmxZ7PUnS6PxEriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWTsoZ/kmiTfTzKbZOe4ry9JPRtr6Cc5B/hn4FrgMuC9SS4bZx8kqWfjfqd/JTBbVU9W1a+BfwW2jrkPktStVWO+3jRwZKh8FHjjcIMkO4Adrfg/Sb4/wvUuAX4ywvEvRT2OGfoct2OeYLntd4oLHfefzbdj3KF/VlW1C9i1FOdKcrCqZpbiXC8VPY4Z+hy3Y+7HUo573Ms7x4D1Q+V1rU6SNAbjDv1vApuSbExyLnADsG/MfZCkbo11eaeqTiX5G+BrwDnA3VX12DJeckmWiV5iehwz9Dlux9yPJRt3qmqpziVJ+iPnJ3IlqSOGviR1ZCJDv4evekiyPslDSR5P8liSm1v9RUkeSPJEe16z0n1dDknOSfLtJF9q5Y1JDrQ5/3y7UWBiJFmdZG+S7yU5nORNPcx1kg+1n+9Hk9yb5OWTONdJ7k5yIsmjQ3VnnN8M3NHG/0iSKxZyrYkL/Y6+6uEU8OGqugzYDNzUxrkT2F9Vm4D9rTyJbgYOD5VvA26vqtcAzwHbV6RXy+fTwFer6nXAGxiMfaLnOsk08AFgpqpez+DmjxuYzLn+LHDNaXXzze+1wKb22AHcuZALTVzo08lXPVTV8ar6Vtv+OYMQmGYw1j2t2R7g+pXp4fJJsg54O3BXKwe4GtjbmkzUuJNcCLwZ2A1QVb+uqufpYK4Z3GH4iiSrgPOB40zgXFfVN4BnT6ueb363AvfUwMPA6iSXvthrTWLon+mrHqZXqC9jkWQDcDlwAFhbVcfbrqeBtSvUreX0KeAjwG9b+WLg+ao61cqTNucbgTngM21J664kFzDhc11Vx4BPAD9mEPYngUNM9lwPm29+R8q4SQz9riR5JfBF4INV9bPhfTW4H3ei7slN8g7gRFUdWum+jNEq4Argzqq6HPgFpy3lTOhcr2HwrnYj8CrgAn5/CaQLSzm/kxj63XzVQ5KXMQj8z1XVfa36mRd+1WvPJ1aqf8vkKuCdSX7EYOnuagbr3avbEgBM3pwfBY5W1YFW3svgRWDS5/qtwA+raq6qfgPcx2D+J3muh803vyNl3CSGfhdf9dDWsXcDh6vqk0O79gHb2vY24P5x9205VdXHqmpdVW1gMLcPVtX7gIeAd7VmEzXuqnoaOJLkta1qC/A4Ez7XDJZ1Nic5v/28vzDuiZ3r08w3v/uAG9tdPJuBk0PLQGdXVRP3AK4D/hv4AfD3K92fZRrjXzL4de8R4DvtcR2D9e39wBPAfwAXrXRfl/Hf4C3Al9r2nwP/BcwC/wact9L9W+Kx/gVwsM33vwNrephr4B+A7wGPAv8CnDeJcw3cy+DvFr9h8Jvd9vnmFwiDOxR/AHyXwd1NL/pafg2DJHVkEpd3JEnzMPQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR/4XPcikNpFAba0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPQyAPXvxwTO"
      },
      "source": [
        "\n",
        "BATCH_SIZE = 32\n",
        "N_CLASSES = 100\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        horizontal_flip=True,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2\n",
        "        )  \n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    )\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow(x_train, y_train)\n",
        "validation_generator = valid_datagen.flow(x_valid, y_valid)\n",
        "test_generator = test_datagen.flow(x_test, y_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xms91hpsyRgA"
      },
      "source": [
        "teacher_model = tf.keras.models.Sequential([\n",
        "                                            WideResidualNetwork(N_CLASSES, 28, 1, includeActivation=False),\n",
        "                                            tf.keras.layers.Activation('softmax')]\n",
        "                                            )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDM_EVciynab"
      },
      "source": [
        "teacher_callback = tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
        "teacher_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKe_g4v2y6TA",
        "outputId": "6d52b8b2-1b79-49cb-bdf0-d4efe2ac0035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "history = teacher_model.fit(\n",
        "    train_generator,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=100,\n",
        "    callbacks=[teacher_callback],\n",
        "    validation_data=validation_generator\n",
        "    \n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 1.9309 - accuracy: 0.4732 - val_loss: 2.3111 - val_accuracy: 0.4103\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.8656 - accuracy: 0.4888 - val_loss: 2.3598 - val_accuracy: 0.4154\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.8099 - accuracy: 0.5006 - val_loss: 2.3263 - val_accuracy: 0.4238\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 1.7590 - accuracy: 0.5131 - val_loss: 2.0167 - val_accuracy: 0.4683\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.7178 - accuracy: 0.5199 - val_loss: 2.0956 - val_accuracy: 0.4568\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.6687 - accuracy: 0.5340 - val_loss: 1.9887 - val_accuracy: 0.4800\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.6395 - accuracy: 0.5424 - val_loss: 2.3972 - val_accuracy: 0.4223\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.5952 - accuracy: 0.5524 - val_loss: 2.1143 - val_accuracy: 0.4740\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.5641 - accuracy: 0.5597 - val_loss: 2.2395 - val_accuracy: 0.4606\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.5346 - accuracy: 0.5682 - val_loss: 1.9031 - val_accuracy: 0.5042\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.5160 - accuracy: 0.5681 - val_loss: 2.6038 - val_accuracy: 0.4245\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.4838 - accuracy: 0.5765 - val_loss: 2.0661 - val_accuracy: 0.4822\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.4496 - accuracy: 0.5865 - val_loss: 1.9410 - val_accuracy: 0.5121\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.4206 - accuracy: 0.5960 - val_loss: 1.8265 - val_accuracy: 0.5241\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.3986 - accuracy: 0.6009 - val_loss: 1.8137 - val_accuracy: 0.5311\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 1.3853 - accuracy: 0.6027 - val_loss: 1.8352 - val_accuracy: 0.5320\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 1.3687 - accuracy: 0.6058 - val_loss: 2.0321 - val_accuracy: 0.5056\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 1.3484 - accuracy: 0.6117 - val_loss: 1.9636 - val_accuracy: 0.5156\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 1.3206 - accuracy: 0.6203 - val_loss: 1.8681 - val_accuracy: 0.5243\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 1.3043 - accuracy: 0.6234 - val_loss: 1.8901 - val_accuracy: 0.5263\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 1.2899 - accuracy: 0.6248 - val_loss: 1.8235 - val_accuracy: 0.5437\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 1.2713 - accuracy: 0.6295 - val_loss: 1.8616 - val_accuracy: 0.5302\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 1.2599 - accuracy: 0.6324 - val_loss: 1.8654 - val_accuracy: 0.5373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_rdupzKzf7S"
      },
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "teacher_model.save_weights('teacher_logits.h5')\n",
        "teacher_model.evaluate(test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_GrENhu21rk"
      },
      "source": [
        "## Knowledge Distillation with logits\n",
        "One possibility to distil knowledge is to use logits and try to match them, the loss function in this case is Mean Squared Error. The way is to remove the Activation layer from the teacher, then, the build_student function has to return a student without softmax and the compile_args dictionary should have the loss key set as \"mse\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUMsMUmo5kM2"
      },
      "source": [
        "teacher_logits = teacher_model.layers[0]\n",
        "teacher_logits.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy9DdY_Y5_1A"
      },
      "source": [
        "build_student = lambda : WideResidualNetwork(N_CLASSES, 28, 1, includeActivation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgy4unXJ6jZC"
      },
      "source": [
        "EPOCHS = 100\n",
        "fit_args= dict(\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, monitor='loss', restore_best_weights=True)],\n",
        "    steps_per_epoch=len(x_train)//BATCH_SIZE\n",
        ")\n",
        "\n",
        "compile_args=dict(\n",
        "    optimizer='adam',\n",
        "    loss='mse'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM503nPk6NTV"
      },
      "source": [
        "history, students = ban(teacher_logits, 2, build_student, train_generator, fit_args=fit_args, compile_args=compile_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuCVMa_27MpZ"
      },
      "source": [
        "complete_students = []\n",
        "\n",
        "for student in students:\n",
        "  complete_students.append(\n",
        "      tf.keras.models.Sequential([\n",
        "                                  student,\n",
        "                                  tf.keras.layers.Activation('softmax')\n",
        "      ])\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBia50-LPzW2",
        "outputId": "df2a1807-500e-4b08-90e4-6d3478f6109c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for student in complete_students:\n",
        "  student.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  student.evaluate(test_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.7794 - accuracy: 0.5404\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.7520 - accuracy: 0.5347\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.8542 - accuracy: 0.5017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izor4HWl4ufn"
      },
      "source": [
        "## Knowledge distillation with softened logits\n",
        "\n",
        "This technique involves softening the softmax output using a temperature T, the idea is to use al Lambda layer after the logits and then pass the output to the softmax. Below there is the implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqBOx8qLoxMS"
      },
      "source": [
        "soft_compile_args = dict(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics='accuracy'\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "013F5wRinhjc"
      },
      "source": [
        "T = 3\n",
        "EPOCHS = 150\n",
        "fit_args= dict(\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=8, monitor='loss', restore_best_weights=True)],\n",
        "    steps_per_epoch=len(x_train)//BATCH_SIZE\n",
        ")\n",
        "\n",
        "softened_logits_teacher = tf.keras.Sequential(\n",
        "    [\n",
        "     teacher_logits,\n",
        "     tf.keras.layers.Lambda(lambda x: x/T),\n",
        "     tf.keras.layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "build_softened_logits_student = lambda : tf.keras.models.Sequential(\n",
        "    [\n",
        "     WideResidualNetwork(N_CLASSES, 28, 1, includeActivation=False),\n",
        "     tf.keras.layers.Lambda(lambda x: x/T),\n",
        "     tf.keras.layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "softened_logits_teacher.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU5XBoRjocMr"
      },
      "source": [
        "history_soft, students_soft = ban(softened_logits_teacher, 2, build_softened_logits_student, train_generator, fit_args=fit_args, compile_args=soft_compile_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTbQzMTwA5av"
      },
      "source": [
        "unsoftened_students = [ tf.keras.models.Sequential(\n",
        "    [student.layers[0],\n",
        "     tf.keras.layers.Activation('softmax')]\n",
        ") for student in students_soft]\n",
        "\n",
        "for student in unsoftened_students:\n",
        "  student.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  student.evaluate(test_generator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6yvlc45nB6"
      },
      "source": [
        "## Knowledge distillation with ground truth\n",
        "Another technique uses the ground truth to adjust the values to match, the method \"ban\" have a parameter called ground_truth_weight, where you can set the weight you want on ground truth, the paper suggests to use low values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRY1AFMeRi84"
      },
      "source": [
        "EPOCHS = 100\n",
        "fit_args= dict(\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=8, monitor='loss', restore_best_weights=True)],\n",
        "    steps_per_epoch=len(x_train)//BATCH_SIZE\n",
        ")\n",
        "\n",
        "compile_gt_args = dict(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "build_student_with_gt = lambda : tf.keras.models.Sequential(\n",
        "    [\n",
        "     WideResidualNetwork(N_CLASSES, 28, 1, includeActivation=False),\n",
        "     tf.keras.layers.Activation('softmax')\n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9KFleo0SL4k",
        "outputId": "90bc11a5-99da-4a51-c8ff-5ffc8b6dbd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_gt, students_gt = ban(teacher_model, \n",
        "                                  1, \n",
        "                                  build_student_with_gt, \n",
        "                                  train_generator, \n",
        "                                  fit_args=fit_args, \n",
        "                                  compile_args=compile_gt_args, \n",
        "                                  ground_truth_weight=1/N_CLASSES)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training BAN-1\n",
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 3.9344 - accuracy: 0.1068\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 3.2780 - accuracy: 0.2239\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 2.8385 - accuracy: 0.3212\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 2.5582 - accuracy: 0.3977\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 2.3711 - accuracy: 0.4470\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 2.2329 - accuracy: 0.4926\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 2.1413 - accuracy: 0.5271\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 2.0738 - accuracy: 0.5466\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 2.0153 - accuracy: 0.5668\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.9826 - accuracy: 0.5784\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.9529 - accuracy: 0.5933\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.9220 - accuracy: 0.6038\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.9043 - accuracy: 0.6133\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.8847 - accuracy: 0.6200\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.8622 - accuracy: 0.6266\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.8511 - accuracy: 0.6333\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.8423 - accuracy: 0.6346\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.8280 - accuracy: 0.6431\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.8212 - accuracy: 0.6435\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.8115 - accuracy: 0.6485\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.8010 - accuracy: 0.6540\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.7880 - accuracy: 0.6605\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7872 - accuracy: 0.6633\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.7778 - accuracy: 0.6645\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.7707 - accuracy: 0.6671\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7676 - accuracy: 0.6683\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7623 - accuracy: 0.6710\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7533 - accuracy: 0.6770\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.7488 - accuracy: 0.6795\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.7463 - accuracy: 0.6773\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7484 - accuracy: 0.6787\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7329 - accuracy: 0.6837\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7344 - accuracy: 0.6814\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.7293 - accuracy: 0.6906\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.7262 - accuracy: 0.6896\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.7198 - accuracy: 0.6893\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7219 - accuracy: 0.6927\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.7155 - accuracy: 0.6939\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.7141 - accuracy: 0.6942\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7152 - accuracy: 0.6919\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7090 - accuracy: 0.6911\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7073 - accuracy: 0.6948\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7045 - accuracy: 0.6987\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.7015 - accuracy: 0.6978\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.7039 - accuracy: 0.7001\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6980 - accuracy: 0.7027\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6975 - accuracy: 0.7042\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6954 - accuracy: 0.7057\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6885 - accuracy: 0.7064\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6843 - accuracy: 0.7068\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6901 - accuracy: 0.7026\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6895 - accuracy: 0.7037\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6808 - accuracy: 0.7093\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6863 - accuracy: 0.7072\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 54s 44ms/step - loss: 1.6831 - accuracy: 0.7084\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.6790 - accuracy: 0.7081\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.6787 - accuracy: 0.7120\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6839 - accuracy: 0.7083\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.6765 - accuracy: 0.7104\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6730 - accuracy: 0.7139\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 1.6755 - accuracy: 0.7138\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6717 - accuracy: 0.7171\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6706 - accuracy: 0.7166\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6654 - accuracy: 0.7169\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6683 - accuracy: 0.7155\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6697 - accuracy: 0.7146\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6626 - accuracy: 0.7185\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6634 - accuracy: 0.7161\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6665 - accuracy: 0.7161\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6617 - accuracy: 0.7215\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6560 - accuracy: 0.7200\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6564 - accuracy: 0.7193\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6590 - accuracy: 0.7205\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6582 - accuracy: 0.7214\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6592 - accuracy: 0.7208\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6491 - accuracy: 0.7223\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6556 - accuracy: 0.7223\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6536 - accuracy: 0.7228\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6496 - accuracy: 0.7192\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6493 - accuracy: 0.7236\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6488 - accuracy: 0.7277\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.6475 - accuracy: 0.7233\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6425 - accuracy: 0.7254\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6463 - accuracy: 0.7293\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6461 - accuracy: 0.7268\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6488 - accuracy: 0.7227\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6462 - accuracy: 0.7233\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6456 - accuracy: 0.7290\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6410 - accuracy: 0.7322\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6397 - accuracy: 0.7284\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6407 - accuracy: 0.7306\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6422 - accuracy: 0.7279\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6389 - accuracy: 0.7328\n",
            "Epoch 94/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6339 - accuracy: 0.7320\n",
            "Epoch 95/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6391 - accuracy: 0.7316\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6355 - accuracy: 0.7315\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6368 - accuracy: 0.7312\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6349 - accuracy: 0.7313\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 1.6318 - accuracy: 0.7349\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6309 - accuracy: 0.7349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1npHA9IMcZOq",
        "outputId": "dc22d1aa-bb5d-4578-b05a-44e5cc3996dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for student in students_gt:\n",
        "  student.evaluate(test_generator)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.8131 - accuracy: 0.5283\n",
            "313/313 [==============================] - 1s 5ms/step - loss: 1.6875 - accuracy: 0.5376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeAN0uHfUF0U",
        "outputId": "9bbdeceb-ab72-4624-b98f-7d911dbc20e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1eef93920fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    }
  ]
}