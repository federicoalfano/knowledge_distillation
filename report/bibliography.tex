\begin{thebibliography}{6}


\bibitem{ban} Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent
Itti, and Anima Anandkumar, ‘Born again neural networks’, arXiv
preprint arXiv:1805.04770, (2018).
\bibitem{kd} Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv:1503.02531, 2015.
\bibitem{fokd}Yim, J., Joo, D., Bae, J., and Kim, J. A gift from knowledge
distillation: Fast optimization, network minimization and
transfer learning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 7130–
7138, 2017.
\bibitem{tkd}Tan, S., Caruana, R., Hooker, G., and Gordo, A. Transparent
model distillation. arXiv:1801.08640, 2018
\bibitem{wresnet} Zagoruyko, S. and Komodakis, N. Wide residual networks.
In Proceedings of the British Machine Vision Conference
(BMVC), pp. 87.1-87.12, 2016b.
\bibitem{conf} C. Buciluˇa, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the
12th ACM SIGKDD International Conference on Knowledge
\bibitem{deepresnet}Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.
\end{thebibliography}
